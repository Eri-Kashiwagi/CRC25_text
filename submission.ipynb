{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library and path imports\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "sys.path.append(\".\")\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shapely.ops as so\n",
    "import shapely.geometry as sg\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import contextily as cx\n",
    "import networkx as nx\n",
    "import momepy\n",
    "import random as rd\n",
    "import random\n",
    "# Local or project-specific imports\n",
    "from utils.helper import get_modified_edges_df\n",
    "from router import Router\n",
    "from utils.graph_op import graphOperator\n",
    "from utils.dataparser import  create_network_graph, handle_weight, handle_weight_with_recovery\n",
    "from utils.metrics import common_edges_similarity_route_df_weighted, get_virtual_op_list\n",
    "\n",
    "import multiprocessing as mp\n",
    "from copy import deepcopy \n",
    "from shapely import wkt\n",
    "from utils.mthread import generate_neighbor_p, parallel_generate_neighbor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LS():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "    \n",
    "    def reset(self):\n",
    "        self.heuristic = self.args['heuristic']\n",
    "        self.heuristic_f = self.args['heuristic_f']\n",
    "        self.attrs_variable_names = self.args['attrs_variable_names']\n",
    "        self.jobs = self.args['jobs']\n",
    "        if self.jobs > 1:\n",
    "            self.pool = mp.Pool(processes=self.jobs)\n",
    "        else:\n",
    "            self.pool = None\n",
    "        \n",
    "\n",
    "        self.df, self.path_foil, self.df_path_foil, self.gdf_coords_loaded = read_data(self.args)\n",
    "        self.user_model = self.args[\"user_model\"]\n",
    "        self.meta_map = self.args[\"meta_map\"]\n",
    "        df_copy = deepcopy(self.df)\n",
    "        df_copy,self.maxx_weight = handle_weight(df_copy, self.user_model)\n",
    "        _, self.G = create_network_graph(df_copy)\n",
    "        self.df=df_copy\n",
    "\n",
    "        self.router_h= Router(heuristic=self.heuristic, CRS=self.meta_map[\"CRS\"], CRS_map=self.meta_map[\"CRS_map\"])\n",
    "        self.graph_operator = graphOperator()\n",
    "        self.origin_node, self.dest_node, self.origin_node_loc, self.dest_node_loc, self.gdf_coords = self.router_h.set_o_d_coords(self.G, self.gdf_coords_loaded)\n",
    "\n",
    "        self.path_fact, self.G_path_fact, self.df_path_fact = self.router_h.get_route(self.G, self.origin_node, self.dest_node, self.heuristic_f)\n",
    "    \n",
    "    def generate_neighbor(self, df):\n",
    "        (df_perturbed_i, G_perturbed_i),(df_path_i, G_path_i), op_list_perturbed = generate_neighbor_p(df, self.router_h, self.graph_operator, self.origin_node, self.dest_node, self.args, self.user_model)\n",
    "        if df_perturbed_i is None:\n",
    "            return (None, None, 0), (None, None, 0), op_list_perturbed\n",
    "        \n",
    "        sub_op_list = get_virtual_op_list(self.df, df_perturbed_i, self.attrs_variable_names)\n",
    "        graph_error = len([op for op in sub_op_list if op[3] == \"success\"])\n",
    "\n",
    "        route_error = 1-common_edges_similarity_route_df_weighted(df_path_i, self.df_path_foil, self.attrs_variable_names)\n",
    "        return (df_perturbed_i, G_perturbed_i, graph_error), (df_path_i, G_path_i, route_error), (op_list_perturbed, sub_op_list)\n",
    "\n",
    "    \n",
    "    def generate_population(self, df, pop_num):\n",
    "        pop = []\n",
    "        if self.jobs > 1:\n",
    "            jobs = [self.pool.apply_async(parallel_generate_neighbor, (df, self.router_h, self.graph_operator, self.origin_node, self.dest_node, self.df_path_foil, self.args, self.user_model, )) for _ in range(pop_num)]\n",
    "            for idx, j in enumerate(jobs):\n",
    "                try:\n",
    "                    (df_perturbed_i, G_perturbed_i, graph_error), (df_path_i, G_path_i, route_error), op_lists = j.get()\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    (df_perturbed_i, G_perturbed_i, graph_error), (df_path_i, G_path_i, route_error), op_lists = (0, 0, 0), (0, 0, 0), []\n",
    "\n",
    "                pop.append(((df_perturbed_i, G_perturbed_i, graph_error), (df_path_i, G_path_i, route_error), op_lists))\n",
    "        else:\n",
    "            for _ in range(pop_num):\n",
    "                pop.append((self.generate_neighbor(df)))\n",
    "        return pop\n",
    "\n",
    "    def get_perturbed_edges(self, df_perturbed):\n",
    "        modified_edges_df = get_modified_edges_df(self.df, df_perturbed, self.attrs_variable_names)\n",
    "        return modified_edges_df\n",
    "\n",
    "\n",
    "def read_data(args):\n",
    "\n",
    "    basic_network_path = args['basic_network_path']\n",
    "    foil_json_path = args['foil_json_path']\n",
    "    df_path_foil_path = args['df_path_foil_path']\n",
    "    gdf_coords_path = args['gdf_coords_path']\n",
    "\n",
    "    df = gpd.read_file(basic_network_path)\n",
    "    with open(foil_json_path, 'r') as f:\n",
    "        path_foil = json.load(f)\n",
    "\n",
    "    df_path_foil = gpd.read_file(df_path_foil_path)\n",
    "    gdf_coords_loaded = pd.read_csv(gdf_coords_path, sep=';')\n",
    "\n",
    "    gdf_coords_loaded['geometry'] = gdf_coords_loaded['geometry'].apply(wkt.loads)\n",
    "    gdf_coords_loaded = gpd.GeoDataFrame(gdf_coords_loaded, geometry='geometry')\n",
    "\n",
    "    return df, path_foil, df_path_foil, gdf_coords_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from utils.parallel_utils import eval_edge_top\n",
    "from utils.alns import backtrack_destroy\n",
    "from utils.alns import pick_max_score\n",
    "from utils.alns import pick_min_score\n",
    "from utils.alns import pick_median_score\n",
    "from utils.alns import pick_random\n",
    "from utils.alns import repair_search_delete\n",
    "from utils.alns import repair_search_change_weight\n",
    "from utils.dataparser import store_op_list, load_op_list\n",
    "from utils.graph_op import pertub_with_op_list\n",
    "from utils.dataparser import convert\n",
    "from shapely import to_wkt\n",
    "import json\n",
    "# route_name =  \"demo_walk\"\n",
    "# route_id = \"1\"\n",
    "# basic_network_path = f'./data/train/{route_name}/network_demo_walk_{route_id}'\n",
    "# foil_json_path = f'./data/train/{route_name}/route_nodes_demo_walk_{route_id}.json'\n",
    "# df_path_foil_path = f'./data/train/{route_name}/route_demo_walk_{route_id}'\n",
    "# gdf_coords_path = f'./data/train/{route_name}/route_demo_walk_{route_id}_start_end.csv'\n",
    "# meta_data_path = f'./data/train/{route_name}/metadata_{route_name}_{route_id}.json'\n",
    "# route_name = \"osdpm_0_4\"  # 当前routes子文件夹\n",
    "# route_id = \"0\"             # 如果有编号你自己补上\n",
    "\n",
    "# basic_network_path = f'./data/train/maps/osdpm_segment_{route_id}.gpkg'  # 路网分块\n",
    "# foil_json_path     = f'./data/train/routes/{route_name}/foil_route.json'\n",
    "# df_path_foil_path  = f'./data/train/routes/{route_name}/foil_route.gpkg'  # 路径表\n",
    "# gdf_coords_path    = f'./data/train/routes/{route_name}/route_start_end.csv'\n",
    "# meta_data_path     = f'./data/train/routes/{route_name}/metadata.json'\n",
    "# with open(meta_data_path, 'r') as f:\n",
    "#     meta_data = json.load(f)\n",
    "# ─────── 全局变量 ───────\n",
    "BASE_DIR   = './data/train'\n",
    "MAPS_DIR   = os.path.join(BASE_DIR, 'maps')\n",
    "ROUTES_DIR = os.path.join(BASE_DIR, 'routes')\n",
    "MAP_FMT    = 'osdpm_segment_{}.gpkg'   # 路网文件的命名\n",
    "time_limit=270\n",
    "# ─────── 批量处理 ───────\n",
    "for route_name in sorted(os.listdir(ROUTES_DIR)):\n",
    "    start_time = time.time()\n",
    "    route_folder = os.path.join(ROUTES_DIR, route_name)\n",
    "    if not os.path.isdir(route_folder):\n",
    "        continue\n",
    "    parts = route_name.split('_')\n",
    "    if len(parts) != 3:\n",
    "        print(f\"跳过非标准目录：{route_name}\")\n",
    "        continue\n",
    "    _, seg_id, rid = parts\n",
    "\n",
    "    # —— 构造各类文件路径 —— #\n",
    "    basic_network_path = os.path.join(MAPS_DIR, MAP_FMT.format(seg_id))\n",
    "    foil_json_path     = os.path.join(route_folder, 'foil_route.json')\n",
    "    df_path_foil_path  = os.path.join(route_folder, 'foil_route.gpkg')\n",
    "    gdf_coords_path        = os.path.join(route_folder, 'route_start_end.csv')\n",
    "    meta_data_path     = os.path.join(route_folder, 'metadata.json')\n",
    "    with open(meta_data_path, 'r') as f:\n",
    "        meta_data = json.load(f)\n",
    "    # Profile settings\n",
    "    user_model = meta_data[\"user_model\"]\n",
    "    meta_map = meta_data[\"map\"]\n",
    "\n",
    "    attrs_variable_names = user_model[\"attrs_variable_names\"]\n",
    "    route_error_delta = user_model[\"route_error_threshold\"]\n",
    "    # Demo route\n",
    "\n",
    "    n_perturbation = 50\n",
    "    operator_p = [0.15, 0.15, 0.15, 0.15, 0.4]\n",
    "    args = {\n",
    "        'basic_network_path': basic_network_path,\n",
    "        'foil_json_path': foil_json_path,\n",
    "        'df_path_foil_path': df_path_foil_path,\n",
    "        'gdf_coords_path': gdf_coords_path,\n",
    "        'heuristic': 'dijkstra',\n",
    "        'heuristic_f': 'my_weight',\n",
    "        'jobs': 1,\n",
    "        'attrs_variable_names': attrs_variable_names,\n",
    "        \"n_perturbation\": n_perturbation,\n",
    "        \"operator_p\": operator_p,\n",
    "        \"user_model\": user_model,\n",
    "        \"meta_map\": meta_map\n",
    "    }\n",
    "\n",
    "    # 1.1 实例化 LS 并 reset，内部已经调用 handle_weight 和 create_network_graph\n",
    "    ls = LS(args)\n",
    "    ls.reset()\n",
    "\n",
    "    # 1.2 从 LS 拿到图 G（有向、多重图），以及原始的 DF 和 foil 路径 DF\n",
    "    G = ls.G  # 由 create_network_graph(handle_weight(df)) 生成，图上每条边上的属性已包含 my_weight\n",
    "\n",
    "    iteration = 0\n",
    "    change1=True\n",
    "    change2=True\n",
    "    change3=True\n",
    "    change4=True\n",
    "    iteration = 0\n",
    "    ls.df[\"edge_idx\"] = ls.df.index\n",
    "    df_perturbed = ls.df.copy()\n",
    "    df_perturbed1=ls.df.copy()\n",
    "\n",
    "    df_full_idx = df_perturbed.set_index(\"geometry\", drop=False)\n",
    "    df_foil_idx = ls.df_path_foil.set_index(\"geometry\")\n",
    "    full_only_cols = df_full_idx.columns.difference(df_foil_idx.columns)\n",
    "    maxx_weight=df_perturbed['my_weight'].abs().max()\n",
    "    # 只把这些 “独有” 列拼过来\n",
    "    df_foil_all = df_foil_idx.join(\n",
    "        df_full_idx[full_only_cols],\n",
    "        how=\"left\"\n",
    "    ).reset_index(drop=True)\n",
    "    _, df_G1 = create_network_graph(df_perturbed)\n",
    "    for idx, foil_row in ls.df_path_foil.iterrows():\n",
    "        # idx 是 ls.df_path_foil 里这一行的索引\n",
    "        foil_geom = foil_row[\"geometry\"]\n",
    "\n",
    "        # 在完整网络表 ls.df 里匹配相同 geometry，拿到对应行号列表\n",
    "        orig_matches = ls.df.index[ls.df[\"geometry\"] == foil_geom].tolist()\n",
    "\n",
    "        if len(orig_matches) == 0 or len(orig_matches) > 1:\n",
    "            continue\n",
    "\n",
    "        orig_idx = orig_matches[0]\n",
    "\n",
    "        # 把找到的 orig_idx 写回到 ls.df_path_foil 的 edge_idx 列\n",
    "        ls.df_path_foil.loc[idx, \"edge_idx\"] = orig_idx\n",
    "\n",
    "        # 以下如果想还原 df_perturbed 中这条边的 include/属性，也可以用 orig_idx 去操作 df_perturbed\n",
    "        if df_perturbed.loc[orig_idx, \"include\"] == 0:\n",
    "            if df_perturbed.loc[orig_idx, \"curb_height_max\"] > user_model[\"max_curb_height\"]:\n",
    "                df_perturbed.loc[orig_idx, \"curb_height_max\"] = user_model[\"max_curb_height\"]\n",
    "                iteration += 1\n",
    "\n",
    "            if df_perturbed.loc[orig_idx, \"obstacle_free_width_float\"] < user_model[\"min_sidewalk_width\"]:\n",
    "                df_perturbed.loc[orig_idx, \"obstacle_free_width_float\"] = user_model[\"min_sidewalk_width\"]\n",
    "                iteration += 1\n",
    "\n",
    "            df_perturbed.loc[orig_idx, \"include\"] = 1\n",
    "    df_path_foil = ls.df_path_foil \n",
    "    ls.df=df_perturbed.copy()\n",
    "    G_con_dir, G_sel_con_dir = create_network_graph(df_perturbed)\n",
    "    ls.G = G_sel_con_dir\n",
    "    df_G=ls.G\n",
    "    edge_index_map = {}\n",
    "    for u, v, key, data in ls.G.edges(keys=True, data=True):\n",
    "        idx = data.get(\"edge_idx\")\n",
    "        if idx is not None:\n",
    "            edge_index_map[idx] = (u, v, key)\n",
    "    fact_path, G_fact_path, df_fact_path = ls.router_h.get_route(\n",
    "        ls.G, ls.origin_node, ls.dest_node, ls.heuristic_f\n",
    "    )\n",
    "    ttt_G=ls.G.copy()\n",
    "    sim_old = common_edges_similarity_route_df_weighted(df_fact_path, ls.df_path_foil, attrs_variable_names)\n",
    "    df_fact_path1=df_fact_path.copy()\n",
    "    route_error_old = 1.0 - sim_old\n",
    "    if route_error_old <= route_error_delta:\n",
    "        # …如果你想要在这里输出最终结果，可以补上\n",
    "        exit(0)\n",
    "    # 进入到正式的迭代循环\n",
    "    last_route_error=route_error_old\n",
    "    last_weight=df_fact_path[\"my_weight\"].sum()\n",
    "    cnt=0\n",
    "    gen_log  = []   # 每次对某条边扰动后的记录\n",
    "    best_log = []   # 每轮选出的最佳扰动记录\n",
    "    new_route_error=None\n",
    "    all_iteration_status=[]\n",
    "    all_iteration_status.append({\n",
    "    \"iteration\":     0,\n",
    "    \"edge_index\":    None,     # 没有被删改的那一行\n",
    "    \"change_df_row\": {},       # 空 dict，表示没有列要更新\n",
    "    \"last_error\":          route_error_old,\n",
    "    \"fact_path\":     df_fact_path\n",
    "    })\n",
    "    while True:\n",
    "        foil_edge_idxs = set(df_foil_all[\"edge_idx\"])\n",
    "        fact_edge_idxs = set(df_fact_path[\"edge_idx\"])\n",
    "        # 2. 计算三类边\n",
    "        common_edges      = foil_edge_idxs & fact_edge_idxs        # 交集：既在 foil 也在 fact 中的边\n",
    "        foil_only_edges   = foil_edge_idxs - fact_edge_idxs        # 只在 foil 中出现的边\n",
    "        fact_only_edges   = fact_edge_idxs - foil_edge_idxs        # 只在 fact 中出现的边\n",
    "        df_common = df_perturbed.loc[list(common_edges)]\n",
    "        df_foil_only = df_perturbed.loc[list(foil_only_edges)]\n",
    "        df_fact_only = df_perturbed.loc[list(fact_only_edges)]\n",
    "        # ---------- 贪心删边 ----------\n",
    "        best_score = -float(\"inf\")\n",
    "        best_idx = None\n",
    "        best_fact_path = None\n",
    "        best_df = None\n",
    "        if(new_route_error!=None):\n",
    "            last_route_error=new_route_error\n",
    "        inputs = [\n",
    "            (\n",
    "                idx,                  # 1. idx\n",
    "                df_perturbed,         # 2. df_perturbed\n",
    "                df_fact_path,         # 3. df_fact_path\n",
    "                df_path_foil,      # 4. df_path_foil\n",
    "                user_model,           # 5. user_model\n",
    "                last_route_error,     # 6. last_route_error\n",
    "                attrs_variable_names, # 7. attrs_variable_names\n",
    "                ls.origin_node,       # 8. origin_node\n",
    "                ls.dest_node,         # 9. dest_node\n",
    "                ls.router_h.get_route, # 10. router_fun\n",
    "                df_G,\n",
    "                edge_index_map,\n",
    "                route_error_delta,\n",
    "            )\n",
    "            for idx in fact_only_edges\n",
    "        ]\n",
    "\n",
    "        if args[\"jobs\"] > 1:\n",
    "            with mp.Pool(processes=args[\"jobs\"]) as pool:\n",
    "                results = pool.starmap(eval_edge_top, inputs)\n",
    "        else:\n",
    "            results = [eval_edge_top(*inp) for inp in inputs]\n",
    "\n",
    "        valid = [r for r in results if r is not None]\n",
    "        if not valid:\n",
    "            break\n",
    "\n",
    "        best_score, best_idx, change_df_row, best_fact_path, best_err = max(\n",
    "            valid, key=lambda x: x[0]\n",
    "        )\n",
    "        if best_idx is None:\n",
    "            break\n",
    "\n",
    "        # 真正生效\n",
    "        df_perturbed.loc[best_idx, change_df_row.keys()] = list(change_df_row.values())\n",
    "        df_fact_path = best_fact_path\n",
    "        new_route_error=best_err\n",
    "        u, v, k = edge_index_map[best_idx]\n",
    "        df_G.remove_edge(u, v, k)\n",
    "        if df_G.has_edge(v,u,k):\n",
    "            flag1=1\n",
    "            df_G.remove_edge(v,u, k)\n",
    "        cnt += 1\n",
    "        ttt=time.time()\n",
    "        if best_err <= route_error_delta:\n",
    "            break\n",
    "        all_iteration_status.append({\n",
    "            \"iteration\":cnt,\n",
    "            \"change_df_row\": change_df_row,\n",
    "            \"edge_index\":best_idx,\n",
    "            \"last_error\":best_err,\n",
    "            \"fact_path\":best_fact_path\n",
    "        })\n",
    "        if(time.time()-start_time>time_limit):\n",
    "            break\n",
    "    cnt2=0\n",
    "    last_route_error=route_error_old\n",
    "    last_weight=df_fact_path1[\"my_weight\"].sum()\n",
    "    new_route_error=None\n",
    "    ungood=0\n",
    "    while cnt>=cnt2:\n",
    "        tt=time.time()\n",
    "        foil_edge_idxs = set(df_foil_all[\"edge_idx\"])\n",
    "        fact_edge_idxs = set(df_fact_path1[\"edge_idx\"])\n",
    "        # 2. 计算三类边\n",
    "        common_edges      = foil_edge_idxs & fact_edge_idxs        # 交集：既在 foil 也在 fact 中的边\n",
    "        foil_only_edges   = foil_edge_idxs - fact_edge_idxs        # 只在 foil 中出现的边\n",
    "        fact_only_edges   = fact_edge_idxs - foil_edge_idxs        # 只在 fact 中出现的边\n",
    "        # 3. 打印一下规模，确认无误\n",
    "        df_common = df_perturbed1.loc[list(common_edges)]\n",
    "        df_foil_only = df_perturbed1.loc[list(foil_only_edges)]\n",
    "        df_fact_only = df_perturbed1.loc[list(fact_only_edges)]\n",
    "        # ---------- 贪心删边 ----------\n",
    "        best_score = -float(\"inf\")\n",
    "        best_idx = None\n",
    "        best_fact_path = None\n",
    "        best_df = None\n",
    "        if(new_route_error!=None):\n",
    "            last_route_error=new_route_error\n",
    "        inputs = [\n",
    "            (\n",
    "                idx,                  # 1. idx\n",
    "                df_perturbed1,         # 2. df_perturbed\n",
    "                df_fact_path1,         # 3. df_fact_path\n",
    "                df_path_foil,      # 4. df_path_foil\n",
    "                user_model,           # 5. user_model\n",
    "                last_route_error,     # 6. last_route_error\n",
    "                attrs_variable_names, # 7. attrs_variable_names\n",
    "                ls.origin_node,       # 8. origin_node\n",
    "                ls.dest_node,         # 9. dest_node\n",
    "                ls.router_h.get_route, # 10. router_fun\n",
    "                df_G1,\n",
    "                edge_index_map,\n",
    "                route_error_delta,\n",
    "            )\n",
    "            for idx in fact_only_edges\n",
    "        ]\n",
    "\n",
    "        if args[\"jobs\"] > 1:\n",
    "            with mp.Pool(processes=args[\"jobs\"]) as pool:\n",
    "                results = pool.starmap(eval_edge_top, inputs)\n",
    "        else:\n",
    "            results = [eval_edge_top(*inp) for inp in inputs]\n",
    "\n",
    "        valid = [r for r in results if r is not None]\n",
    "        if not valid:\n",
    "            break\n",
    "\n",
    "        best_score, best_idx, change_df_row, best_fact_path, best_err = max(\n",
    "            valid, key=lambda x: x[0]\n",
    "        )\n",
    "        if best_idx is None:\n",
    "            break\n",
    "        # 真正生效\n",
    "        df_perturbed1.loc[best_idx, change_df_row.keys()] = list(change_df_row.values())\n",
    "        df_fact_path1 = best_fact_path\n",
    "        new_route_error=best_err\n",
    "        u, v, k = edge_index_map[best_idx]\n",
    "        df_G1.remove_edge(u, v, k)\n",
    "        if df_G1.has_edge(v,u,k):\n",
    "            flag1=1\n",
    "            df_G1.remove_edge(v,u, k)\n",
    "        cnt2 += 1\n",
    "        ttt=time.time()\n",
    "        if best_err <= route_error_delta:\n",
    "            ungood=1\n",
    "            break\n",
    "        if(time.time()-start_time>time_limit):\n",
    "            break\n",
    "    min_iteration=cnt+iteration\n",
    "    route_error_min=-999999\n",
    "    ddd=0\n",
    "    if(cnt!=1):\n",
    "        new_route_error=None\n",
    "\n",
    "        #---------------------------------------\n",
    "        #进行ALNS破除局部\n",
    "        T_init = 130      # 初始温度\n",
    "        alpha  = 0.996      # 降温速度\n",
    "        T_min  = 10        # 最小温度\n",
    "        b      = 0.5       # 算子权重更新系数\n",
    "        c      = 0.05      # backtrack 权重函数中的调节参数\n",
    "\n",
    "        num_destroy = 4\n",
    "        num_repair  = 8\n",
    "\n",
    "        # 破坏算子\n",
    "        wDestroy      = [1.0]*num_destroy\n",
    "        imp_sum_destroy   = [0.0] * num_destroy\n",
    "        imp_count_destroy = [0]   * num_destroy\n",
    "        subScore_destroy  = [0.0] * num_destroy\n",
    "        destroy_operators=[0,0.25,0.5,0.75]\n",
    "        destroyUseTime=[0.0] * num_destroy\n",
    "\n",
    "        wRepair       = [1.0]*num_repair\n",
    "        repairUseTime = [0]*num_repair\n",
    "        repairScore   = [1.0]*num_repair\n",
    "        Repair_son_sub=[0.2,0.2,0.2,0.4]\n",
    "        #八个修复算子\n",
    "        num_subops  = 8                 # 子算子总数\n",
    "        wSub        = [1.0] * num_subops  # 权重（轮盘赌用）\n",
    "        subUseTime  = [0]   * num_subops  # 使用次数\n",
    "        subScore    = [1.0] * num_subops  # 积分（>0 保证第一次除数不为 0）\n",
    "        last_subUseTime=subUseTime.copy()\n",
    "        b_sub       = 0.7                # 子算子权重平滑系数\n",
    "\n",
    "        fullCalls   = [0] * num_subops   # accept==2 的使用次数\n",
    "        annealCalls = [0] * num_subops   # accept==1 的使用次数\n",
    "        rejCalls    = [0] * num_subops   # accept==0 的使用次数\n",
    "        # 主循环\n",
    "        T = T_init\n",
    "        ddd=0\n",
    "        base_score=cnt+iteration\n",
    "        best_iteration=[]\n",
    "        for rec in all_iteration_status:\n",
    "            best_iteration.append({\n",
    "                \"iteration\":     rec[\"iteration\"],\n",
    "                \"change_df_row\": rec[\"change_df_row\"].copy(),  # dict.copy()\n",
    "                \"edge_index\":    rec[\"edge_index\"],\n",
    "                \"last_error\":    rec[\"last_error\"],\n",
    "                \"fact_path\":     rec[\"fact_path\"].copy()       # DataFrame.copy()\n",
    "            })\n",
    "        weight_history = []\n",
    "        weight_history1=[]\n",
    "        count_history  = []\n",
    "        while T > T_min:\n",
    "            ddd+=1\n",
    "            df0= ls.df.copy()\n",
    "            G=ttt_G.copy()\n",
    "            destroy_idx = random.choices(range(len(destroy_operators)), weights=wDestroy, k=1)[0]\n",
    "            ratio = destroy_operators[destroy_idx]\n",
    "            chosen_idx,now_iteration = backtrack_destroy(\n",
    "                history = all_iteration_status,\n",
    "                ratio=ratio,\n",
    "                df0     = df0,\n",
    "                c       = c,\n",
    "                G=G,\n",
    "                edge_index_map=edge_index_map,\n",
    "            )\n",
    "            slice_hist=[]\n",
    "            for h in all_iteration_status[:chosen_idx+1]:\n",
    "                slice_hist.append({\n",
    "                    \"iteration\":      h[\"iteration\"],\n",
    "                    \"change_df_row\":  h[\"change_df_row\"].copy(),    # dict浅拷贝\n",
    "                    \"edge_index\":     h[\"edge_index\"],\n",
    "                    \"last_error\":     h[\"last_error\"],\n",
    "                    \"fact_path\":      h[\"fact_path\"].copy(),        # DataFrame.copy()\n",
    "                })\n",
    "            # _,G = create_network_graph(df0)\n",
    "            v_op_list = get_virtual_op_list(ls.df, df0, args[\"attrs_variable_names\"])\n",
    "            count=0\n",
    "            df_fact_path=all_iteration_status[chosen_idx][\"fact_path\"]\n",
    "            last_route_error=all_iteration_status[chosen_idx][\"last_error\"]\n",
    "            while(True):\n",
    "                new_record=0\n",
    "                foil_edge_idxs = set(df_foil_all[\"edge_idx\"])\n",
    "                fact_edge_idxs = set(df_fact_path[\"edge_idx\"])\n",
    "                # 2. 计算三类边\n",
    "                common_edges      = foil_edge_idxs & fact_edge_idxs        # 交集：既在 foil 也在 fact 中的边\n",
    "                foil_only_edges   = foil_edge_idxs - fact_edge_idxs        # 只在 foil 中出现的边\n",
    "                fact_only_edges   = fact_edge_idxs - foil_edge_idxs        # 只在 fact 中出现的边\n",
    "                # 3. 打印一下规模，确认无误\n",
    "                df_common = df0.loc[list(common_edges)]\n",
    "                df_foil_only = df0.loc[list(foil_only_edges)]\n",
    "                df_fact_only = df0.loc[list(fact_only_edges)]\n",
    "                best_score = -float(\"inf\")\n",
    "                best_idx = None\n",
    "                best_fact_path = None\n",
    "                best_df = None\n",
    "                if(new_route_error!=None):\n",
    "                    last_route_error=new_route_error\n",
    "                op_idx = random.choices(range(num_subops), weights=wSub, k=1)[0]\n",
    "                if op_idx > 3:\n",
    "                    df_foil_only_temp = df0.loc[list(foil_only_edges)]\n",
    "                    df_foil_only_temp = df_foil_only_temp[\n",
    "                        df_foil_only_temp['path_type'] != user_model['walk_bike_preference']\n",
    "                    ]\n",
    "                    if len(df_foil_only_temp) == 0:\n",
    "                        # 强制切换到删边算子 (0-3)\n",
    "                        op_idx = random.choices(range(4), weights=wSub[:4], k=1)[0]\n",
    "                if op_idx <= 3:\n",
    "                    inputs = [\n",
    "                    (\n",
    "                        idx,                  # 1. idx\n",
    "                        df0,         # 2. df_perturbed\n",
    "                        user_model,           # 5. user_model\n",
    "                    )\n",
    "                    for idx in fact_only_edges\n",
    "                    ]\n",
    "                    results = [repair_search_delete(*inp) for inp in inputs]\n",
    "                    valid = [r for r in results if r is not None]\n",
    "                    valid_sorted = sorted(valid, key=lambda x: x[0], reverse=True)\n",
    "                    pickers = [pick_max_score, pick_min_score, pick_median_score, pick_random]\n",
    "                    chosen_op = pickers[op_idx]\n",
    "                    # 3) 用它从 valid 里挑出 (score, idx, change, fact_path, err)\n",
    "                    best_score, best_idx, change_df_row= chosen_op(valid)\n",
    "                    df0.loc[best_idx, change_df_row.keys()] = list(change_df_row.values())\n",
    "                    u, v, k = edge_index_map[best_idx]\n",
    "                    G.remove_edge(u, v, k)\n",
    "                    if G.has_edge(v,u,k):\n",
    "                        G.remove_edge(v,u, k)\n",
    "                    _, _, best_fact_path = ls.router_h.get_route(\n",
    "                        G, ls.origin_node, ls.dest_node, ls.heuristic_f\n",
    "                    )\n",
    "                    sim_old = common_edges_similarity_route_df_weighted(best_fact_path, ls.df_path_foil, attrs_variable_names)\n",
    "                    best_err=1-sim_old\n",
    "                    # 4) 应用改动到 df_curr，得到 df_new\n",
    "                    # 真正生效\n",
    "                    df_fact_path = best_fact_path\n",
    "                    new_route_error=best_err\n",
    "                    subUseTime[op_idx]+=1\n",
    "                    count += 1\n",
    "\n",
    "                else:\n",
    "                    foil_edge_useful_idxs = set(df_foil_only_temp[\"edge_idx\"])\n",
    "                    inputs = [\n",
    "                    (\n",
    "                        idx,                  # 1. idx\n",
    "                        df0,         # 2. df_perturbed\n",
    "                        user_model,           # 5. user_model\n",
    "                        ls.maxx_weight,\n",
    "                    )\n",
    "                    for idx in foil_edge_useful_idxs\n",
    "                    ]\n",
    "                    results = [repair_search_change_weight(*inp) for inp in inputs]\n",
    "                    valid = [r for r in results if r is not None]\n",
    "                    valid_sorted = sorted(valid, key=lambda x: x[0], reverse=True)\n",
    "                    pickers = [pick_max_score, pick_min_score, pick_median_score, pick_random]\n",
    "                    chosen_op = pickers[op_idx-4]\n",
    "\n",
    "                    # 3) 用它从 valid 里挑出 (score, idx, change, fact_path, err)\n",
    "                    best_score, best_idx, change_df_row = chosen_op(valid)\n",
    "                    df0.loc[best_idx, change_df_row.keys()] = list(change_df_row.values())\n",
    "                    u, v, k = edge_index_map[best_idx]\n",
    "                    G[u][v][k][\"my_weight\"]=change_df_row[\"my_weight\"]\n",
    "                    if G.has_edge(v,u,k):\n",
    "                        G[u][v][k][\"my_weight\"]=change_df_row[\"my_weight\"]\n",
    "                    _, _, best_fact_path = ls.router_h.get_route(\n",
    "                        G, ls.origin_node, ls.dest_node, ls.heuristic_f\n",
    "                    )\n",
    "                    sim_old = common_edges_similarity_route_df_weighted(best_fact_path, ls.df_path_foil, attrs_variable_names)\n",
    "                    best_err=1-sim_old\n",
    "                    # 4) 应用改动到 df_curr，得到 df_new\n",
    "                    # 真正生效\n",
    "                    df_fact_path = best_fact_path\n",
    "                    new_route_error=best_err\n",
    "                    count += 1\n",
    "                    subUseTime[op_idx]+=1\n",
    "                if best_err <= route_error_delta:\n",
    "                    break\n",
    "                slice_hist.append({\n",
    "                    \"iteration\":count+now_iteration,\n",
    "                    \"change_df_row\": change_df_row,\n",
    "                    \"edge_index\":best_idx,\n",
    "                    \"last_error\":best_err,\n",
    "                    \"fact_path\":best_fact_path\n",
    "                })\n",
    "                # all_iteration_status[:] = slice_hist\n",
    "\n",
    "            # — c) 评估\n",
    "            now_score=count+now_iteration+iteration\n",
    "            if base_score>now_score:\n",
    "                all_iteration_status[:] = slice_hist\n",
    "                base_score=now_score\n",
    "                accept = 2\n",
    "                subScore_destroy[destroy_idx]+=1\n",
    "                if(len(best_iteration)>len(all_iteration_status)):\n",
    "                    min_iteration=count+now_iteration+iteration\n",
    "                    best_iteration.clear()\n",
    "                    for rec in all_iteration_status:\n",
    "                        best_iteration.append({\n",
    "                            \"iteration\":     rec[\"iteration\"],\n",
    "                            \"change_df_row\": rec[\"change_df_row\"].copy(),  # dict.copy()\n",
    "                            \"edge_index\":    rec[\"edge_index\"],\n",
    "                            \"last_error\":    rec[\"last_error\"],\n",
    "                            \"fact_path\":     rec[\"fact_path\"].copy()       # DataFrame.copy()\n",
    "                        })\n",
    "                    sim_old = common_edges_similarity_route_df_weighted(df_fact_path, ls.df_path_foil, attrs_variable_names)\n",
    "                    route_error_min = 1.0 - sim_old\n",
    "                    df_perturbed=df0.copy()\n",
    "                    subScore_destroy[destroy_idx]+=5\n",
    "                    new_record=2\n",
    "                if(len(best_iteration)==len(all_iteration_status)):\n",
    "                    best_iteration.clear()\n",
    "                    for rec in all_iteration_status:\n",
    "                        best_iteration.append({\n",
    "                            \"iteration\":     rec[\"iteration\"],\n",
    "                            \"change_df_row\": rec[\"change_df_row\"].copy(),  # dict.copy()\n",
    "                            \"edge_index\":    rec[\"edge_index\"],\n",
    "                            \"last_error\":    rec[\"last_error\"],\n",
    "                            \"fact_path\":     rec[\"fact_path\"].copy()       # DataFrame.copy()\n",
    "                        })\n",
    "                    subScore_destroy[destroy_idx]+=3\n",
    "                    new_record=1\n",
    "\n",
    "            else:\n",
    "                p = np.exp(15*(base_score-now_score-1) / T)\n",
    "                if rd.random() < p:\n",
    "                    accept = 1\n",
    "                    subScore_destroy[destroy_idx]+=0.1\n",
    "                    all_iteration_status[:] = slice_hist\n",
    "                    base_score+=(now_score-base_score)/2\n",
    "                else:\n",
    "                    accept = 0\n",
    "                    subScore_destroy[destroy_idx]-=0.1\n",
    "                    \n",
    "            gamma = 0.08   # 惩罚系数\n",
    "            eps   = 1e-6\n",
    "            tau0_d       = 1.0\n",
    "            min_tau_d    = 0.1\n",
    "            mu0_d        = 0.3\n",
    "            mu_min_d     = 0.05\n",
    "            MAX_ITERS    = 600\n",
    "            # 平滑更新：用 (1–b)*平均得分 + b*旧权重\n",
    "            # destroyScore 用 subScore_destroy 累积打分\n",
    "            # 1) 进度与温度\n",
    "            progress = min(1.0, ddd / float(MAX_ITERS))\n",
    "            tau_d    = max(min_tau_d, tau0_d * (1 - progress))\n",
    "\n",
    "            # 2) 清洗 subScore_destroy，防止 NaN/负值\n",
    "            eps_score = 1e-6\n",
    "            subScore_destroy = [\n",
    "                eps_score if (not np.isfinite(s) or s < eps_score) else s\n",
    "                for s in subScore_destroy\n",
    "            ]\n",
    "\n",
    "            # 3) 数值稳定版 Softmax\n",
    "            scores     = np.array(subScore_destroy, dtype=float)\n",
    "            max_score  = np.max(scores)\n",
    "            shifted    = (scores - max_score) / tau_d\n",
    "            exp_scores = np.exp(shifted)               # 全部 <= 1\n",
    "            sum_exp    = exp_scores.sum() or 1.0\n",
    "            w_norm_d   = (exp_scores / sum_exp).tolist()\n",
    "\n",
    "            # 4) 混合探索：保留 μ 均匀概率\n",
    "            mu     = mu0_d * (1 - progress) + mu_min_d * progress\n",
    "            u_prob = 1.0 / num_destroy\n",
    "            wDestroy = [(1 - mu) * w + mu * u_prob for w in w_norm_d]\n",
    "\n",
    "            # 5) 归一化一次，消除浮点误差\n",
    "            s = sum(wDestroy) or 1.0\n",
    "            wDestroy = [wd / s for wd in wDestroy]\n",
    "\n",
    "            weight_history1.append(wDestroy.copy())\n",
    "\n",
    "            # —— 3) 按“全接受／退火接受／拒绝”比率更新 subScore —— \n",
    "            for i in range(num_subops):\n",
    "                calls = subUseTime[i]\n",
    "                if accept == 2:\n",
    "                    fullCalls[i]   += calls\n",
    "                elif accept == 1:\n",
    "                    annealCalls[i] += calls\n",
    "                else:\n",
    "                    rejCalls[i]    += calls\n",
    "                p_full   = fullCalls[i] \n",
    "                p_anneal = annealCalls[i] \n",
    "                p_reject = rejCalls[i]  \n",
    "                # reward = 1*p_full + 0.5*p_anneal – γ*p_reject\n",
    "                reward = p_full + 0.07 * p_anneal - gamma * p_reject\n",
    "                if(new_record==2):\n",
    "                    reward+=subUseTime[i]*17\n",
    "                if(new_record==1):\n",
    "                    reward+=subUseTime[i]*5\n",
    "                subScore[i] = max(\n",
    "                    eps,\n",
    "                    (1 - b_sub) * subScore[i]\n",
    "                    + b_sub     * reward\n",
    "                )\n",
    "            \n",
    "            # —— 4) 归一化得到新权重 wSub —— \n",
    "            total = sum(subScore)\n",
    "            wSub   = [s/total for s in subScore]\n",
    "            total = sum(subScore)\n",
    "            if total > 0:\n",
    "                wSub = [s / total for s in subScore]\n",
    "            else:\n",
    "                wSub = [1.0/num_subops] * num_subops\n",
    "\n",
    "            progress = ddd / MAX_ITERS\n",
    "            mu0    = 0.3   # 初始探索强度（30% 均匀探索）\n",
    "            mu_min = 0.05  # 最终保留探索比例（5% 均匀探索）\n",
    "            mu     = mu0 * (1 - progress) + mu_min * progress\n",
    "\n",
    "            # U 是均匀分布\n",
    "            u_prob = 1.0 / num_subops\n",
    "            # 混合\n",
    "            wSub = [(1 - mu) * w + mu * u_prob for w in wSub]\n",
    "            # —— 5) 清空本轮调用统计 —— \n",
    "            subUseTime = [0] * num_subops\n",
    "            weight_history.append(wSub.copy())\n",
    "            count_history.append(count + iteration+now_iteration)\n",
    "            # —— 6) 退火降温 —— \n",
    "            T *= alpha\n",
    "            if(time.time()-start_time>time_limit):\n",
    "                break\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    if(ungood==1):\n",
    "        if(min_iteration>cnt2):\n",
    "            df_perturbed=df_perturbed1\n",
    "    ls.reset()\n",
    "    v_op_list = get_virtual_op_list(ls.df, df_perturbed, args[\"attrs_variable_names\"])\n",
    "    available_op = [(op[0], (convert(op[1][0]), to_wkt(op[1][1], rounding_precision=-1, trim=False)), convert(op[2]), op[3]) for op in v_op_list if op[3] == \"success\"]\n",
    "    #test store and load op list\n",
    "    store_path = \"./outputs/\"\n",
    "    store_op_path = f'{store_path}op_list_{route_name}.json'\n",
    "    with open(store_op_path, 'w') as f:\n",
    "        json.dump(available_op, f)\n",
    "    df_perturbed.to_file(f'{store_path}p_network_{route_name}.gpkg', driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.figure()\n",
    "# for i in range(len(weight_history[0])):\n",
    "#     plt.plot([w[i] for w in weight_history], label=f\"subop {i}\")\n",
    "# plt.xlabel(\"ALNS Outer Iteration\")\n",
    "# plt.ylabel(\"Weight (repair)\")\n",
    "# plt.title(\"Sub-Operator Weights Over Iterations\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# for i in range(len(weight_history1[0])):\n",
    "#     plt.plot([w[i] for w in weight_history], label=f\"subop {i}\")\n",
    "# plt.xlabel(\"ALNS Outer Iteration\")\n",
    "# plt.ylabel(\"Weight (Destory)\")\n",
    "# plt.title(\"Sub-Operator Weights Over Iterations\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # 2) Total perturbations per ALNS run\n",
    "# plt.figure()\n",
    "# plt.plot(count_history)\n",
    "# plt.xlabel(\"ALNS Outer Iteration\")\n",
    "# plt.ylabel(\"Number of Perturbations\")\n",
    "# plt.title(\"Total Perturbations per ALNS Run\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "# # Extract best-only data from your logs\n",
    "# iterations   = [e[0] for e in best_log]\n",
    "# weight_sums  = [e[2] for e in best_log]\n",
    "# similarities = [e[3] for e in best_log]\n",
    "\n",
    "# # Best Weight Sum curve\n",
    "# fig, ax1 = plt.subplots(figsize=(8, 4))\n",
    "# ax1.plot(iterations, weight_sums, marker='s', linestyle='-', label='Best Weight Sum')\n",
    "# ax1.set_xlabel('Iteration')\n",
    "# ax1.set_ylabel('Weight Sum')\n",
    "# ax1.set_title('Best Weight Sum per Iteration')\n",
    "# ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "# ax1.legend()\n",
    "# ax1.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Best Similarity curve\n",
    "# fig, ax2 = plt.subplots(figsize=(8, 4))\n",
    "# ax2.plot(iterations, similarities, marker='s', linestyle='-', label='Best Similarity')\n",
    "# ax2.set_xlabel('Iteration')\n",
    "# ax2.set_ylabel('Similarity')\n",
    "# ax2.set_title('Best Similarity per Iteration')\n",
    "# ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "# ax2.legend()\n",
    "# ax2.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_coords = ls.gdf_coords.copy()\n",
    "# origin_node_loc_length = ls.origin_node_loc\n",
    "# dest_node_loc_length = ls.dest_node_loc\n",
    "\n",
    "# # Subset network for plotting\n",
    "# my_rad = 70\n",
    "# gdf_coords['buffer'] = gdf_coords['geometry'].buffer(my_rad, cap_style=3)\n",
    "# plot_area = gpd.GeoDataFrame(geometry=[gdf_coords['buffer'][0].union(gdf_coords['buffer'][1])], crs=meta_map[\"CRS\"])\n",
    "# df_sub = gpd.sjoin(ls.df, plot_area, how='inner').reset_index()\n",
    "\n",
    "\n",
    "# attrs_color = {\"path_type\": {\"c\":\"yellow\",\"ls\": \"-\", \"lw\": 5}, \n",
    "#                \"curb_height_max\": {\"c\":\"green\",\"ls\": \"-\", \"lw\": 4}, \n",
    "#                \"obstacle_free_width_float\": {\"c\":\"orange\",\"ls\": \"-\", \"lw\": 3}}\n",
    "# fig, ax = plt.subplots(figsize=(12,12))\n",
    "\n",
    "# # Network\n",
    "# df_sub.plot(ax=ax, color='lightgrey', linewidth=1)\n",
    "# ls.df_path_fact.plot(ax=ax, color='grey', linewidth=4)\n",
    "# ls.df_path_foil.plot(ax=ax, color='black', linewidth=4)\n",
    "# df_fact_path.plot(ax=ax, color='green', linewidth=2)\n",
    "# # not_common_edges_df.plot(ax=ax, color='yellow', linewidth=2)\n",
    "# # Origin and destination location\n",
    "# gdf_coords.head(1).plot(ax=ax, color='blue', markersize=50)\n",
    "# gdf_coords.tail(1).plot(ax=ax, color='red', markersize=50)\n",
    "\n",
    "# # Origin and destination nodes\n",
    "# gpd.GeoSeries([origin_node_loc_length], crs=meta_map[\"CRS\"]).plot(ax=ax, color='blue', markersize=20)\n",
    "# gpd.GeoSeries([dest_node_loc_length], crs=meta_map[\"CRS\"]).plot(ax=ax, color='red', markersize=20)\n",
    "\n",
    "# # Background\n",
    "# cx.add_basemap(ax=ax, source=cx.providers.CartoDB.Voyager, crs=meta_map[\"CRS\"])\n",
    "\n",
    "# # Legend\n",
    "# route_acc = mpatches.Patch(color='black', label='foil_route')\n",
    "# route = mpatches.Patch(color='grey', label='fact_route')\n",
    "# route_best = mpatches.Patch(color='green', label='best_route (perturbed)')\n",
    "# origin = mpatches.Patch(color='blue', label= 'Orgin')\n",
    "# dest = mpatches.Patch(color='red', label= 'destination')\n",
    "# legend_handles = [route_acc,route,route_best,origin,dest]\n",
    "# for attr, color in attrs_color.items():\n",
    "#     legend_handles.append(mpatches.Patch(color=color[\"c\"], label=attr))\n",
    "\n",
    "\n",
    "# plt.legend(handles=legend_handles,loc=\"lower right\")\n",
    "\n",
    "# plt.axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standard library and path imports\n",
    "# import sys\n",
    "# import os\n",
    "# import time\n",
    "# import json\n",
    "# sys.path.append(\".\")\n",
    "# # Third-party library imports\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import shapely.ops as so\n",
    "# import shapely.geometry as sg\n",
    "# import geopandas as gpd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as mpatches\n",
    "# import contextily as cx\n",
    "# import networkx as nx\n",
    "# import momepy\n",
    "# import random as rd\n",
    "# import random\n",
    "# # Local or project-specific imports\n",
    "# from utils.helper import get_modified_edges_df\n",
    "# from router import Router\n",
    "# from utils.graph_op import graphOperator\n",
    "# from utils.dataparser import  create_network_graph, handle_weight, handle_weight_with_recovery\n",
    "# from utils.metrics import common_edges_similarity_route_df_weighted, get_virtual_op_list\n",
    "\n",
    "# import multiprocessing as mp\n",
    "# from copy import deepcopy \n",
    "# from shapely import wkt\n",
    "# from utils.mthread import generate_neighbor_p, parallel_generate_neighbor\n",
    "\n",
    "# outputs_root = r\"C:\\Users\\Lenovo\\Desktop\\crc标准版\\CRC25_text\\outputs\"\n",
    "# for seg in range(5):\n",
    "#     for rid in range(1, 6):\n",
    "#         route_name = f\"osdpm_{seg}_{rid}\"\n",
    "#         op_file    = os.path.join(outputs_root, f\"op_list_{route_name}.json\")\n",
    "#         net_file   = os.path.join(outputs_root, f\"p_network_{route_name}.gpkg\")\n",
    "\n",
    "#         if not os.path.exists(op_file) or not os.path.exists(net_file):\n",
    "#             print(f\"跳过 {route_name}：文件缺失\")\n",
    "#             continue\n",
    "\n",
    "#         # 1) 读回 op_list\n",
    "#         with open(op_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#             raw_ops = json.load(f)\n",
    "#         # 只保留成功标记的操作\n",
    "#         available_ops = [op for op in raw_ops if op[3] == \"success\"]\n",
    "#         n_ops = len(available_ops)\n",
    "#         print(f\"\\n=== {route_name} 共 {n_ops} 条操作 ===\")\n",
    "# # —— 你的目录设置 —— \n",
    "# outputs_root     = r\"C:\\Users\\Lenovo\\Desktop\\crc标准版\\CRC25_text\\outputs\"\n",
    "# routes_data_root = r\"C:\\Users\\Lenovo\\Desktop\\crc标准版\\CRC25_text\\data\\train\\routes\"\n",
    "\n",
    "# # 遍历 seg=0..4, rid=1..5\n",
    "# for seg in range(5):\n",
    "#     for rid in range(1, 6):\n",
    "#         route_name = f\"osdpm_{seg}_{rid}\"\n",
    "\n",
    "#         # 输出目录下的文件\n",
    "#         op_file    = os.path.join(outputs_root,   f\"op_list_{route_name}.json\")\n",
    "#         net_file   = os.path.join(outputs_root,   f\"p_network_{route_name}.gpkg\")\n",
    "#         route_file = os.path.join(outputs_root,   f\"p_route_{route_name}.gpkg\")\n",
    "\n",
    "#         # 原始 foil 路径 & metadata\n",
    "#         foil_gpkg   = os.path.join(routes_data_root, route_name, \"foil_route.gpkg\")\n",
    "#         meta_json   = os.path.join(routes_data_root, route_name, \"metadata.json\")\n",
    "\n",
    "#         # 检查文件是否都存在\n",
    "#         if not (os.path.exists(op_file) and os.path.exists(net_file)\n",
    "#                 and os.path.exists(route_file) and os.path.exists(foil_gpkg)\n",
    "#                 and os.path.exists(meta_json)):\n",
    "#             print(f\"跳过 {route_name}：文件不全\")\n",
    "#             continue\n",
    "\n",
    "#         # 1) 读 metadata 拿到 attrs_variable_names\n",
    "#         with open(meta_json, \"r\", encoding=\"utf-8\") as f:\n",
    "#             meta = json.load(f)\n",
    "#         attrs_variable_names = meta[\"user_model\"][\"attrs_variable_names\"]\n",
    "\n",
    "#         # 2) 读入最终的 network 和 route\n",
    "#         df_net   = gpd.read_file(net_file)\n",
    "#         df_route = gpd.read_file(route_file)\n",
    "#         df_foil  = gpd.read_file(foil_gpkg)\n",
    "\n",
    "#         # 3) 打印 network 与 route 的基本信息\n",
    "#         print(f\"\\n--- {route_name} ---\")\n",
    "#         print(f\"Network: {len(df_net)} 条边;  Route: {len(df_route)} 条边\")\n",
    "\n",
    "#         # 4) 计算与 foil 路径的相似度 & 误差\n",
    "#         sim = common_edges_similarity_route_df_weighted(\n",
    "#             df_route, df_foil, attrs_variable_names\n",
    "#         )\n",
    "#         err = 1.0 - sim\n",
    "#         print(f\"Final router_error = {err:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.dataparser import store_op_list, load_op_list\n",
    "# from utils.graph_op import pertub_with_op_list\n",
    "# from utils.dataparser import convert\n",
    "# from shapely import to_wkt\n",
    "# import json\n",
    "# route_id=\"\"\n",
    "# ls.reset()\n",
    "# v_op_list = get_virtual_op_list(ls.df, df_perturbed, args[\"attrs_variable_names\"])\n",
    "# available_op = [(op[0], (convert(op[1][0]), to_wkt(op[1][1], rounding_precision=-1, trim=False)), convert(op[2]), op[3]) for op in v_op_list if op[3] == \"success\"]\n",
    "# #test store and load op list\n",
    "# store_path = \"./examples/demo_walk/outputs/\"\n",
    "# store_op_path = f'{store_path}op_list_{route_name}_{route_id}.json'\n",
    "# with open(store_op_path, 'w') as f:\n",
    "#     json.dump(available_op, f)\n",
    "# df_fact_path.to_file(f'{store_path}p_route_{route_name}_{route_id}.gpkg', driver='GPKG')\n",
    "# df0.to_file(f'{store_path}p_network_{route_name}_{route_id}.gpkg', driver='GPKG')\n",
    "\n",
    "# route_name = \"osdpm_4_2\"  # 当前routes子文件夹\n",
    "# route_id = \"\"             # 如果有编号你自己补上\n",
    "\n",
    "# basic_network_path = './examples/demo_walk/outputs/p_network_osdpm_4_2_.gpkg'  # 路网分块\n",
    "# foil_json_path     = f'./data/train/routes/{route_name}/foil_route.json'\n",
    "# df_path_foil_path  = f'./data/train/routes/{route_name}/foil_route.gpkg'  # 路径表\n",
    "# gdf_coords_path    = f'./data/train/routes/{route_name}/route_start_end.csv'\n",
    "# meta_data_path     = f'./data/train/routes/{route_name}/metadata.json'\n",
    "# with open(meta_data_path, 'r') as f:\n",
    "#     meta_data = json.load(f)\n",
    "\n",
    "# # Profile settings\n",
    "# user_model = meta_data[\"user_model\"]\n",
    "# meta_map = meta_data[\"map\"]\n",
    "\n",
    "# attrs_variable_names = user_model[\"attrs_variable_names\"]\n",
    "# route_error_delta = user_model[\"route_error_threshold\"]\n",
    "# # Demo route\n",
    "\n",
    "\n",
    "# #perturbation\n",
    "# n_perturbation = 50\n",
    "# operator_p = [0.15, 0.15, 0.15, 0.15, 0.4]\n",
    "\n",
    "# args = {\n",
    "#     'basic_network_path': basic_network_path,\n",
    "#     'foil_json_path': foil_json_path,\n",
    "#     'df_path_foil_path': df_path_foil_path,\n",
    "#     'gdf_coords_path': gdf_coords_path,\n",
    "#     'heuristic': 'dijkstra',\n",
    "#     'heuristic_f': 'my_weight',\n",
    "#     'jobs': 10,\n",
    "#     'attrs_variable_names': attrs_variable_names,\n",
    "#     \"n_perturbation\": n_perturbation,\n",
    "#     \"operator_p\": operator_p,\n",
    "#     \"user_model\": user_model,\n",
    "#     \"meta_map\": meta_map\n",
    "# }\n",
    "\n",
    "\n",
    "# best_weighted_error = 1000000\n",
    "# best_graph_error = 1000\n",
    "# best_route_error = 1000\n",
    "# gen_num = 10000\n",
    "# lagrangian_lambda = 2000\n",
    "\n",
    "# start_time = time.time()\n",
    "# time_limit = 60*4\n",
    "\n",
    "\n",
    "# ls = LS( args)\n",
    "# ls.reset()\n",
    "\n",
    "# best_df = [ls.df.copy()]\n",
    "# best_route = [None]\n",
    "# best_log = []\n",
    "# gen_log = []\n",
    "\n",
    "# # compare fact and foil route\n",
    "# fact_path, G_fact_path, df_fact_path  = ls.router_h.get_route(ls.G, ls.origin_node, ls.dest_node, ls.heuristic_f)\n",
    "# route_similarity = common_edges_similarity_route_df_weighted(df_fact_path, ls.df_path_foil, ls.attrs_variable_names)\n",
    "# print(\"error of fact route and foil route\", 1-route_similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
